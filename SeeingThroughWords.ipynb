{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cadairhinojosa/AI-Image-Captioning-for-the-Visually-Impaired-/blob/main/SeeingThroughWords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYlbkwoFX42A"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch torchvision --quiet\n",
        "!pip install ultralytics --quiet\n",
        "!pip install gradio --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "import gradio as gr\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "Owdu2SLNZUNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model_caption = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n"
      ],
      "metadata": {
        "id": "PLPZq1RUZg3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_yolo = YOLO(\"yolov8n.pt\")  # Use the lightweight version\n"
      ],
      "metadata": {
        "id": "9hFf8xGFZqEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "danger_keywords = [\n",
        "    'fire', 'gun', 'knife', 'weapon', 'explosion', 'smoke',\n",
        "    'car', 'truck', 'bus', 'train', 'crowd', 'bear', 'dog',\n",
        "    'cliff', 'motorcycle', 'police car', 'rain', 'drugs','police light', 'danger sign',\n",
        "    'blood', 'crime scene', 'police', 'handcuffs', 'broken', 'glass', 'ambulance', 'crime', 'fight'\n",
        "]\n"
      ],
      "metadata": {
        "id": "9yfAVSQNZuPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(image):\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    out = model_caption.generate(**inputs)\n",
        "    return processor.decode(out[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "aGLzHutJfgl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_danger(image):\n",
        "    results = model_yolo(image)\n",
        "    labels = []\n",
        "    for r in results:\n",
        "        labels += [model_yolo.names[int(c)] for c in r.boxes.cls]\n",
        "\n",
        "    matched = [item for item in labels if item.lower() in danger_keywords]\n",
        "\n",
        "    # Check for specific crime scene keywords first, excluding 'police'\n",
        "    crime_scene_keywords = ['gun', 'knife', 'blood', 'fight', 'handcuffs','police']\n",
        "    if any(word in matched for word in crime_scene_keywords):\n",
        "        return f\"ðŸ”´ Possible Crime Scene: {', '.join(set(matched))}\"\n",
        "    # Check if 'police' is present without other crime scene keywords\n",
        "    elif 'police' in matched and not any(word in matched for word in crime_scene_keywords):\n",
        "        return f\"ðŸŸ¡ Caution: {', '.join(set(matched))}\"\n",
        "    # Check for other danger keywords\n",
        "    elif any(word in matched for word in danger_keywords if word not in crime_scene_keywords and word != 'police'):\n",
        "         return f\"ðŸŸ¡ Caution: {', '.join(set(matched))}\"\n",
        "    else:\n",
        "        return \"ðŸŸ¢ Safe\""
      ],
      "metadata": {
        "id": "_wgBHSEnfi_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_image(img):\n",
        "    caption = generate_caption(img)\n",
        "    danger = detect_danger(img)\n",
        "\n",
        "    # Optional: flag certain keywords from the caption\n",
        "    crime_words = ['arrest', 'weapon', 'blood', 'shooting']\n",
        "    if any(word in caption.lower() for word in crime_words):\n",
        "        danger = \"ðŸ”´ Possible Crime Scene (based on caption)\"\n",
        "\n",
        "    return f\"Caption: {caption}\\n\\nSafety Status: {danger}\"\n",
        "\n",
        "    return f\"Caption: {caption}\\n\\nSafety Status: {danger}\"\n"
      ],
      "metadata": {
        "id": "pnEMxkA4flAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.Interface(\n",
        "    fn=analyze_image,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"ðŸ§  Crime Scene Caption & Detection App\",\n",
        "    description=\"Upload an image to receive a description and a danger warning.\"\n",
        ").launch()\n"
      ],
      "metadata": {
        "id": "-M4KIASMPbGc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "2f1b85d8-d887-4267-ddcc-960fe10c285f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://19edfde82fb0ad51d4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://19edfde82fb0ad51d4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ]
}